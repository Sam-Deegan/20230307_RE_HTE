---
title: "Replication_Group_1"
short: "A shorter title"
journal: "AER" # AER, AEJ, PP, JEL
month: "`r format(Sys.Date(), '%m')`"
year: "`r format(Sys.Date(), '%Y')`"
vol: 1
issue: 1
jel:
  - A10
  - A11
keywords:
  - first keyword
  - second keyword
author:
  - name: Alice Anonymous
    firstname: Alice
    surname: Anonymous
    email: alice@example.com
    affiliation: Some Institute of Technology
  - name: Bob Security
    firstname: Bob
    surname: Security
    email: bob@example.com
    affiliation: Another University
acknowledgements: |
  Acknowledgements
abstract: |
  Abstract goes here
output:  rticles::aea_article
---

\section{Introduction}

In this paper, we replicate "Liquidity Constraints, Informal Institutions, and the Adoption of Weather Insurance" by Belissa et al. (2019) and extend it to account for heterogeneous treatment effects via a Multi-Arm-Causal-Forest model. 
The original study finds that drought insurance uptake among smallholder farmers in Ethiopia could be higher, as smallholders suffer from seasonal liquidity constraints associated with harvest periods. The paper builds upon previous work in the literature, which finds that downside production risks impede farm modernisation (Emerick et al., 2016). By introducing a novel insurance product which delays repayment (IOUs) until after the harvest, the authors find that insurance uptake can be considerably increased. 
The authors conducted a Randomised Control Trial (RCT) in 2015 using a pre-existing index-based drought-insurance product offered by Oromia Insurance Company (OIC) in the Ethiopia Rift Valley area. The product is typically sold during the rainy season before the harvest, and payouts are determined based on local rainfall levels.
To improve the effectiveness and uptake of the insurance product, the authors made three modifications to the original product. Firstly, they allowed deferred IOU payment post-harvest to alleviate smallholder liquidity constraints. Secondly, they varied the market channel and advertised the product through local (Iddir) leaders to increase trust and information. Finally, they imposed harsher contract terms to reduce the likelihood of default. In total, six different index-based insurance contracts were offered. The availability of these modified insurance products was randomised at the Iddir level to test their effectiveness. 
Our extension examines whether increases in insurance uptake are consistent with the author’s theory that financial constraints limit uptake. If true, one would expect increased drought insurance uptake among poorer smallholders. The study's findings have implications for policymakers and stakeholders working to improve the financial resilience of smallholder farmers in developing countries and increase smallholder investment in new technologies.
The design of the original papers’ novel insurance product is motivated by the findings of two previous studies, which it builds upon. 
The first study, by Casaburi and Willis (2018), studies delayed payments of the premium to induce insurance uptake when insurance is interlinked with a contract farming scheme. This prevents defaults on the premium payment commitments as premiums are deducted from the revenues paid after harvest (but contract enforcement issues remain because of the risk of side-selling).
The second study, by Dercon et al. (2014), investigates the effectiveness of marketing through informal local groups to boost insurance uptake. Additionally, they consider the impact of delaying insurance premiums on insurance uptake. Belissa et al. (2019) test both measures in several combinations to examine their effects on insurance updates and defaults. 
The findings of Belissa et al. (2019) share similarities with Casaburi and Willis (2018) outside a contract farming context. The study finds that the demand-increasing effect of the IOUs may be more significant for players with low savings or income, in line with Dercon et al. (2014). The result suggests that liquidity constraints impede the uptake of drought-based insurance. By building on and extending previous research, the study provides a more nuanced understanding of the factors that affect insurance uptake among smallholder farmers in developing countries.
Our heterogeneity analysis focuses on two household characteristics, household income and savings. The authors undertake limited heterogeneity analysis in the paper, comparing below and above median income and participants with and without savings. We take a more nuanced approach using machine learning to calculate heterogeneity across the income and savings spectrum. Specifically, we implement a Multi Arm Causal Forest model developed by Wager and Athey (2017) to estimate these effects across several treatments. This extension aims to determine whether the author’s theory that liquidity issues hamper drought-based insurance uptake among agricultural smallholders in Ethiopia is consistent with the results. 

By analyzing data using causal inference methods, the study reveals that modifying insurance could benefit individuals with higher incomes. Additionally, the study identifies that the iddir marketing channel disproportionately affects individuals with higher incomes, which has significant implications for the design of marketing strategies for insurance programs in rural Ethiopia. Overall, the study offers insights for developing more effective insurance programs in rural areas and highlights the potential benefits of using causal inference techniques to explore complex issues in development economics.

The paper is structured into six sections. The first section is the introduction, which overviews the study and its objectives. The second section is the literature review, which discusses the research on the topic and identifies the research gaps the study aims to address. The third section is the methodology, which describes the data sources, study design, and statistical techniques used to analyse the data. The fourth section is the results, which present the study findings. The fifth section, the extension, discusses the multi-arm causal forest approach used to estimate heterogeneous treatment effects. The sixth and final section is the conclusion, summarising the study's key findings and their implications for policy and future research.

\section{Methodology}

This subsection discusses the methodology used to replicate Belissa et al. (2019). We provide a detailed explanation of the data and methods used in the original study, including any necessary assumptions or model specifications. We also explain the steps to ensure our replication accurately reproduces the original study's results.

\subsection{Data}
As established in the introduction, the authors randomise the availability of several alternative index-based insurance contracts at the Iddir-level to test the effect of their design on insurance uptake and premium defaults.
The data gathered by the authors is cross-sectional in structure. All observations date to 2015? and contains 8,579 household observations. Six variations of index-based drought insurance were offered to smallholders. These are (1) Standard Index Insurance (control), (2) Standard Index Insurance via \textit{Iddir} promotions, (3) IOU insurance, (4) IOU insurance with Contract, (5) IOU insurance via \textit{Iddir} promotions; and (6) IOU insurance via \textit{Iddir} promotions with Contract. The availability of these treatments/products was randomised over 144 \textit{Iddirs}. In addition to randomising treatment the researchers collected data on socio-economic status , farm production  and savings to test whether randomisation was successful and to facilitate the estimation of causal effects. 

\subsection{Replication}
The authors employed a multi-arm Randomized Control Trial (RCT) methodology to estimate the causal effects of their novel insurance products on index-based insurance uptake among smallholders. In contrast to traditional two-arm RCTs, a multi-arm RCT compares more than two intervention groups to a control group, in this case, five. Each intervention group receives a different treatment, and the primary objective is to identify the most effective treatment(s), with the control group serving as the baseline comparison. Compared to two-arm RCTs, multi-arm RCTs offer several benefits, including increased study efficiency, potential identification of multiple effective treatments, and decreased risk of false-negative results. However, multi-arm RCTs can be more challenging due to increased trial design, analysis, and interpretation complexity. As a result, careful planning and execution are necessary to ensure the comparability of intervention arms and the study's ability to detect meaningful differences between the groups.
The authors estimate causal effects for cross-sectional data via ordinary least squares (OLS). This method involves regressing the outcome variable on the treatment variable and any other relevant predictor variables. The coefficient for the treatment variable represents the average difference in the outcome variable between the treatment and control groups, controlling for any other relevant variables.
OLS is a widely used statistical technique often applied in an RCT to estimate the effect of an intervention on an outcome variable. OLS offers several advantages, including its ability to control for other factors that might influence the outcome, making it easier to communicate study results to a broader audience, and providing estimates of treatment effects that can be used to calculate the cost-effectiveness of the intervention.
However, OLS has some disadvantages that should be considered in an RCT. One major drawback is its assumption of a linear relationship between the outcome variable and explanatory variables. The results may be biased or inaccurate if this assumption is not met. OLS also does not account for potential confounding variables that were not included in the regression model, leading to biased estimates of treatment effects. Lastly, OLS requires a large sample size to provide reliable estimates of treatment effects, and if the sample size is too small, the results may be unreliable.

The authors estimate Estimating causal effects for cross-sectional data via ordinary least squares (OLS). This method involves regressing the outcome variable on the treatment variable and any other relevant predictor variables. The coefficient for the treatment variable represents the average difference in the outcome variable between the treatment and control groups, controlling for any other relevant variables.
OLS (Ordinary Least Squares) is a widely used statistical technique often applied in an RCT (Randomized Control Trial) to estimate the effect of an intervention on an outcome variable. OLS offers several advantages, including its ability to control for other factors that might influence the outcome, making it easier to communicate study results to a broader audience, and providing estimates of treatment effects that can be used to calculate the cost-effectiveness of the intervention.
To verify whether randomisation was successful and that treatment is not confounded by other variables balancing tests are required to ensure no significant differences between groups before intervention. If groups differ significantly, it may bias the results making it difficult to mark causal inference. 
However, OLS has some disadvantages that should be considered in an RCT. One major drawback is its assumption of a linear relationship between the outcome variable and explanatory variables. The results may be biased or inaccurate if this assumption is not met. OLS also does not account for potential confounding variables that were not included in the regression model, leading to biased estimates of treatment effects. Lastly, OLS requires a large sample size to provide reliable estimates of treatment effects, and if the sample size is too small, the results may be unreliable.

\subsection{Extension}
We apply a multi-arm causal forest model methodology to extend the original report’s findings to estimate the heterogeneous treatment effects. A multi-arm causal forest is a machine-learning algorithm that can estimate the causal effect of multiple treatments on an outcome variable in a randomised control trial (RCT) setting. It extends the Random Forest algorithm commonly used for prediction tasks.

The multi-arm causal forest model builds on the traditional Random Forest model by incorporating information about the treatment assignments in the RCT. The key idea of a causal forest model is to build causal trees, which are decision trees that partition the data based on the causal effect of a specific feature. These trees can then be aggregated to create a causal forest, which provides an estimate of the treatment effect. A multi-arm causal forest model broadens the base causal forest algorithm to model for multiple treatment groups compared to the control group.

We implement the multi-arm casual forest model Nie and Wager (2021) suggested. This approach extends the causal forest algorithm, a machine learning algorithm that estimates treatment effects by partitioning the data into subgroups based on observed covariates. The quasi-oracle approach extends the causal forest algorithm by using kernel weights to estimate the treatment effect for multiple treatment arms simultaneously.

The equation is given by:
$\tilde{\tau}(x) = \operatorname{argmin}{\tau} \left{ \sum{i=1}^n \alpha_i(x) \left(Y_i - \bar{m}^{(-i)}(X_i) - c(x) - \langle W_i - \hat{e}^{(-i)}(X_i), \tau(X_i) \rangle \right)^2 \right}$

In this equation, $\tilde{\tau}(x)$ represents the estimated conditional average treatment effect for the target sample $x$. The subscript $i$ refers to the $i$th observation in the sample, and $n$ represents the total number of observations. The observed outcomes are denoted by $Y_i$, and the observed covariates by $X_i$. The estimated propensity score is denoted by $\hat{e}(X_i)$, and is a vector-valued function that assigns a probability to each treatment arm, conditional on the observed covariates.

The weights $\alpha_i(x)$ reflect the contribution of each observation to the estimation of the treatment effect for the target sample $x$. The intercept term $c(x)$ is a nuisance parameter that captures any systematic bias in the treatment effect estimates that is not captured by the observed covariates or the estimated propensity score.

The term $\bar{m}^{(-i)}(X_i)$ represents the average outcome for the control group, which is estimated by excluding the $i$th observation from the sample. The notation $(-i)$ indicates that the $i$th observation is excluded from the calculation. Similarly, the term $\hat{e}^{(-i)}(X_i)$ represents the estimated propensity score for the control group, which is also estimated by excluding the $i$th observation from the sample.

The term $\langle W_i - \hat{e}^{(-i)}(X_i), \tau(X_i) \rangle$ represents the estimated treatment effect for each treatment arm, which is a linear combination of the estimated propensity score and the observed covariates. The notation $\langle \cdot, \cdot \rangle$ denotes the inner product of two vectors.

The optimisation problem seeks to find the value of $\tilde{\tau}(x)$ that minimises the sum of squared residuals, given the observed covariates, the estimated propensity score, and the weights $\alpha_i(x)$. The resulting value of $\tilde{\tau}(x)$ represents the estimated treatment effect for each treatment arm, conditional on the observed covariates for the target sample $x$.

However, the quasi-oracle approach has some weaknesses. The approach requires a large sample size for sufficient statistical power, especially when estimating treatment effects for rare subgroups or interactions. Secondly, the approach is computationally intensive and can be time-consuming to implement, especially when simultaneously estimating treatment effects for multiple outcomes. Additionally, the approach may suffer from overfitting and may be sensitive to the choice of hyperparameters. Lastly, the quasi-oracle approach assumes that the treatment assignment mechanism is ignorable, meaning that no unobserved confounding variables affect both the treatment assignment and the outcome. If this assumption is violated, the treatment effect estimates may be biased.

<point out that we have a small number of observations. Overfitting is a potential issues as we have a large number of covariates. We assume no confounders, so balancing must be correct otherwise we have bias.> 


When checking the dataset we obtained from the official resource, we found that the numbers of \textit{Iddirs} of three groups---IBI, IOU and IOU_C, are much larger than the results in the original paper, while the total number of \textit{Iddirs} and the numbers of observations of all groups are consistent. This issue indicates the fact that households in each \textit{Iddir} received different kinds of treatment, which is inconsistent with what the authors state in the paper. And the original paper does not explain whether/how they recategorise the households into new \textit{Iddirs}. We tried to contact the corresponding author, but he could not provide us a clear explanation in time because of some private reasons. So, due to this ambiguity, our randomisation does not work as well as that of the original study, and it leads to regression results that are different from those obtained by the authors.

\section{Results}

This chapter aims to replicate the findings of Belissa et al. (2019) in their study on drought insurance uptake among smallholder farmers in Ethiopia. The original study uses a randomised controlled trial (RCT) to test the effectiveness of modified index-based insurance products, which aim to alleviate liquidity constraints and increase trust among smallholders. Our replication examines the key findings of the original study, including the impact of deferred IOU payment, marketing through local leaders, and harsher contract terms on insurance uptake and defaults. By replicating the study, we aim to verify the robustness of the original findings and contribute to the reproducibility and transparency of research in development economics.

\subsection{Data}

To begin our replication, we first verified the consistency of the randomisation process in our dataset with that reported by Belissa et al. (2019). The original paper states that randomisation occurred at the Iddir level, with 144 iddirs across the treatment and control arms. However, upon examination of the data, we found that the total number of iddirs across treatment arms was 226, with several Iddirs appearing in multiple arms. Only the Standard Insurance via Iddir Promotions seems unaffected. We confirmed that our replication code did not alter the data and obtained a second dataset from another source, which yielded the same result. Thus, we conclude that the replication data was systematically distorted before uploading. We have contacted the original author concerning the paper.

\begin{table}
\centering
\begin{tabular}{l|r|r}
\hline
Uptake1 & Observations & Iddir\\
\hline
standard insurance through the usual channel (coops) & 59 & 15\\
\hline
Standard insurance through iddirs & 457 & 17\\
\hline
IOU insurance through iddirs with BC & 396 & 13\\
\hline
IOU insurance through iddirs without BC & 886 & 17\\
\hline
IOU insurance through the usual channel with BC & 69 & 12\\
\hline
IOU insurance through usual channel without BC & 167 & 14\\
\hline
Non-buyer & 6545 & 138\\
\hline
\end{tabular}
\end{table}

Note that using a distorted dataset will affect our ability to replicate further material from the original paper. In particular, this will affect our calculation of heterogeneous treatment effects in the extension due to the distortion of the randomisation arm, which relies on randomisation for unconfoundedness.

\subsection{Randomisation}

To verify whether randomisation was successful and that treatment is not confounded by other variables balancing tests are required to ensure no significant differences between groups before intervention. If groups differ significantly, it may bias the results making it difficult to mark causal inference.

We then assess the balancing of the randomisation process. The original paper used this to verify the success of the randomisation process for causal inference. We match the author’s approach and regress a battery of socio-economic, production and savings variables on the treatment groups. We do not include the control group, the “Standard Index Insurance” dummy, which is therefore reflected in the constant.

Comparing the constants of our balancing test to Table 1 in the original paper, we attempt to trace the impact of the data distortion. Constants for Age, Sex, Education, Family Size, Monthly Income, Drought Severity and Previous Insurance are unaffected. The marriage dummy is significantly higher at 0.95 compared to 0.90 in the original paper. Point estimates for each independent variable adhere closely to those presented in the original paper. However, they do not match exactly in almost all cases. This would suggest that distortions in the dataset are relatively small.

\begin{table}
\centering
\tiny
\caption*{Balance tests on socio-economic variables.}
\begin{tabular}{llllllllll}
\hline
 & Age & Sex & Marital & Education & Family & Monthly & Drought & Insured\\
 & (years) & (1=male) & status & (years) & size & income & dummy & Before\\
 \hline
Index Insurance via Iddir & -0.867** & 0.114** & -0.018 & 0.050 & 0.123 & -198.243** & 0.018 & -0.051***\\
 & (0.047) & (0.000) & (0.067) & (0.699) & (0.223) & (0.050) & (0.553) & (0.000)\\
 IOU Insurance & -2.317** & 0.012 & -0.017 & 0.405* & -0.228* & -58.499 & -0.208** & -0.090**\\
 & (0.000) & (0.645) & (0.172) & (0.019) & (0.088) & (0.663) & (0.000) & (0.000)\\
IOU Insurance with Contract & -1.124** & 0.159** & -0.024* & 0.058 & 0.403** & 62.845 & -0.032 & -0.031*\\
 & (0.020) & (0.000) & (0.024) & (0.692) & (0.000) & (0.577) & (0.347) & (0.014)\\
IOU Insurance via Iddir & -0.521 & 0.121** & -0.030* & 0.334* & 0.317** & 305.535* & 0.238* & 0.082**\\
 & (0.262) & (0.000) & (0.003) & (0.016) & (0.003) & (0.005) & (0.000) & (0.000)\\
IOU via Iddir with Contract & -1.835** & 0.023 & -0.005 & 0.108 & -0.378* & -160.661 & -0.155* & -0.081**\\
 & (0.002) & (0.378) & (0.687) & (0.541) & (0.006) & (0.241) & (0.000) & (0.000)\\
Constant (Index Insurance) & 39.380** & 0.470* & 0.953* & 1.902* & 5.672* & 854.298* & 4.716* & 0.121**\\
Observations & 8579 & 8579 & 8579 & 8579 & 8579 & 8579 & 8579 & 8579 \\
Wald Tests & & & & & & & & \\
IBI\_Iddir=IOU & 0.00 & 0.00 & 0.11 & 0.04 & 0.00 & 0.03 & 0.00 & 0\\
IBI\_Iddir=IOU\_C & 0.00 & 0.00 & 0.09 & 0.82 & 0.00 & 0.07 & 0.00 & 0\\
IBI\_Iddir=IOU\_Iddir & 0.09 & 0.00 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 & 0\\
IBI\_Iddir=IOU\_Iddir\_C & 0.04 & 0.00 & 0.04 & 0.91 & 0.00 & 0.00 & 0.21 & 0\\
IOU=IOU\_C & 0.00 & 0.68 & 0.34 & 0.07 & 0.01 & 0.36 & 0.00 & 0\\
IOU=IOU\_Iddir & 0.00 & 0.00 & 0.01 & 0.03 & 0.00 & 0.00 & 0.00 & 0\\
IOU=IOU\_Iddir\_C & 0.00 & 0.00 & 0.05 & 0.05 & 0.00 & 0.58 & 0.00 & 0\\
IOU\_C=IOU\_Iddir & 0.00 & 0.00 & 0.00 & 0.04 & 0.00 & 0.00 & 0.00 & 0\\
IOU\_C=IOU\_Iddir\_C & 0.00 & 0.00 & 0.04 & 0.82 & 0.00 & 0.12 & 0.00 & 0\\
IOU\_Iddir=IOU\_Iddir\_C & 0.05 & 0.00 & 0.00 & 0.02 & 0.00 & 0.02 & 0.00 & 0\\
\hline
\end{tabular}
\begin{tabular}{p{0.8\textwidth}}
Notes: Robust standard errors in parentheses, clustered for 144 \textit{Iddirs}; ***p $<$ 0.01, **p $<$ 0.05, *p $<$ 0.1. Wald tests show p-values of equality. The constant reflects the average in the control group: Standard Index Insurance.
\end{tabular}
\end{table}

We test whether the treatments’ coefficients are equal via the Wald test. In general, the results are highly significant except for the education variable. High significant levels would normally lead us to conclude that coefficients are unequal. However, in the context of data distortions noted in the dataset between randomisation and clusters, particularly as we are required to clusters robust errors by iddir, we cannot discount the possibility that our results are distorted.

Distortions of the constants in Table 2 remain small but are more frequent. Only the constant for Wheat (5.09) matches the original paper. All other constants align closely with the original paper but differ somewhat. The increased frequency of departures from the original paper may not represent any distortion pattern, but this data contains more variance, and therefore distortions are more visible.

\begin{table}
\centering
\tiny
\caption*{Table 2: Balance tests for production variables and savings}
\begin{tabular}{llllllllll}
\hline
 & Maize & Haricot & Teff & Sorghum & Wheat & Barley & Land & Savings\\
 \hline
Index Insurance via Iddir & 2.145** & 0.027 & -0.181 & -0.061* & -1.194* & -0.066* & -0.427** & 0.063***\\
 & (0.000) & (0.466) & (0.101) & (0.005) & (0.000) & (0.025) & (0.031) & (0.000)\\
 IOU Insurance & 0.345 & -0.023 & -0.037 & -0.049* & -1.188** & -0.078* & 0.374 & -0.002\\
 & (0.413) & (0.633) & (0.800) & (0.086) & (0.005) & (0.045) & (0.153) & (0.938)\\
IOU Insurance with Contract & 0.403 & 0.022 & 0.016 & -0.043 & -0.859** & -0.071* & 0.022 & -0.035\\
 & (0.349) & (0.651) & (0.917) & (0.137) & (0.048) & (0.076) & (0.934) & (0.118)\\
IOU Insurance via Iddir & 0.508 & -0.023 & 0.026 & -0.009 & 0.595* & -0.092** & -1.189** & 0.017\\
 & (0.134) & (0.548) & (0.824) & (0.678) & (0.082) & (0.003) & (0.000) & (0.343)\\
IOU via Iddir with Contract & 1.625** & 0.093* & -0.248** & -0.030 & -1.048** & -0.026 & 0.877** & 0.010\\
 & (0.000) & (0.022) & (0.044) & (0.209) & (0.003) & (0.427) & (0.000) & (0.591)\\
Constant (Index Insurance) & 6.522** & 0.190* & 1.224* & 0.146* & 5.085* & 0.167* & 7.987* & 1.213**\\
Observations & 8579 & 8579 & 8579 & 8579 & 8579 & 8579 & 8579 & 8579 \\
Wald Tests & & & & & & & & \\
IBI\_Iddir=IOU & 0.00 & 0.30 & 0.21 & 0.03 & 0.00 & 0.14 & 0.00 & 0.00\\
IBI\_Iddir=IOU\_C & 0.00 & 0.76 & 0.15 & 0.03 & 0.00 & 0.17 & 0.03 & 0.00\\
IBI\_Iddir=IOU\_Iddir & 0.00 & 0.15 & 0.03 & 0.00 & 0.00 & 0.03 & 0.00 & 0.00\\
IBI\_Iddir=IOU\_Iddir\_C & 0.00 & 0.08 & 0.16 & 0.01 & 0.00 & 0.09 & 0.00 & 0.00\\
IOU=IOU\_C & 0.56 & 0.59 & 0.95 & 0.18 & 0.01 & 0.14 & 0.37 & 0.18\\
IOU=IOU\_Iddir & 0.29 & 0.81 & 0.89 & 0.17 & 0.00 & 0.04 & 0.00 & 0.47\\
IOU=IOU\_Iddir\_C & 0.00 & 0.02 & 0.09 & 0.23 & 0.01 & 0.11 & 0.00 & 0.78\\
IOU\_C=IOU\_Iddir & 0.29 & 0.48 & 0.98 & 0.27 & 0.00 & 0.04 & 0.00 & 0.02\\
IOU\_C=IOU\_Iddir\_C & 0.00 & 0.09 & 0.06 & 0.31 & 0.02 & 0.26 & 0.00 & 0.06\\
IOU\_Iddir=IOU\_Iddir\_C & 0.00 & 0.01 & 0.01 & 0.45 & 0.00 & 0.01 & 0.00 & 0.62\\
\hline
\end{tabular}
\begin{tabular}{p{0.8\textwidth}}
Notes: Robust standard errors in parentheses, clustered for 144 \textit{Iddirs}; ***p $<$ 0.01, **p $<$ 0.05, *p $<$ 0.1. Wald tests show p-values of equality. The constant reflects the average in the control group: Standard Index Insurance.
\end{tabular}
\end{table}

Wald tests for joint significance between treatment arms in production variables are again extremely significant. These results are substantially more significant than in the original paper. The high significance level likely represents the earlier distortions noted in the dataset between randomisation and clusters, particularly as Iddir clusters robust errors. 

As noted previously regarding Table 1. The results of our Wald test are highly significant. Usually, this would lead us to conclude that the coefficients are significantly different. However, we are reluctant to place faith in the results in light of our data issues.

\subsection{Uptake Rates}

Figure 2 illustrates the insurance uptake rates across various treatment arms. The IOU product's delayed payment option shows a substantial increase in uptake compared to standard insurance, jumping from 8% to 24%. The combination of IOU and promotion through Iddir outperforms all other treatments, with uptake rates reaching approximately 43%. These results are consistent with those reported in the original paper, except for Group 6, which received the most comprehensive intervention package, including IOU, promotion by Iddir leaders, and a binding contract. In our study, the uptake rate of this group was slightly lower than in the original experiment, specifically 27% compared to 32%. However, upon further examination of the data, we discovered that around 5% of households in this group took up an IOU via Iddir without signing a contract, which explains the discrepancy between our findings and those of the authors. Consequently, we have adopted the authors' calculation method in our subsequent research.

![Figure 2: Uptake rates across IOU treatments, 95% CI clustered at Iddir level.](2_Analysis/C_Output/figure_2.png)

The results of regression analysis, shown in Table 3, are also quite similar to what the authors obtained. IOU insurance with and without Iddir promotion both have significant positive effects on the uptake rate. However, introducing a binding contract to the IOU has a chastening effect on uptake rates with and without Iddir promotions, suggesting that much of the additional adoption induced by delayed payment is either motivated by the prospect of strategic default, or the result of farmers who are unsure about their ability to pay the future premium – and thus scared away once a binding contract is introduced – also in the absence of opportunistic intentions. People need a high degree of trust to sign a binding contract—especially when the consequences of signing are possibly not fully understood. The Iddir could assuage such concerns by acting as a trusted third party, resulting in increased uptake rates also in the presence of a contract. However, in our results, Index Insurance via Iddir has a significant impact on the uptake in the parsimonious model. Besides, Wald tests of most coefficient pairs again show extreme significance, substantially with much higher p-values than those in the original paper. This indicates that the effects of different treatment types vary to a large extent.

\begin{table}
\centering
\tiny
\caption*{Table 3: Insurance uptake rates increase under IOU}
\begin{tabular}{llll}
\hline
 & (1) & (2) & (3) \\
 & Parsimonious & Additional & Excluding\\
 & model & controls & Dalota Mati \\
\hline
Index Insurance via Iddir & 0.071*** & 0.012 & 0.005 \\ 
  & (0.016) & (0.014) & (0.014) \\  
IOU Insurance & 0.166** & 0.150* & 0.120** \\ 
  & (0.021) & (0.018) & (0.019) \\ 
IOU Insurance with Contract & 0.033 & 0.030 & 0.011 \\ 
  & (0.021) & (0.018) & (0.019) \\ 
IOU Insurance via Iddir & 0.349** & 0.336* & 0.333** \\ 
  & (0.017) & (0.015) & (0.015) \\ 
IOU via Iddir with Contract & 0.245** & 0.179* & 0.175** \\ 
  & (0.018) & (0.015) & (0.016) \\ 
Constant & 0.077** & -0.349* & -0.299** \\ 
  & (0.014) & (0.038) & (0.040) \\
Additional controls & No & Yes & Yes \\
Kebele fixed effects & No & Yes & Yes \\ 
Iddir clustered s.e. & 144 & 144 & 144 \\
Observations & 8,579 & 8,579 & 7969 \\ 
Adjusted R$^{2}$ & 0.086 & 0.332 & 0.337 \\ 
Wald tests \\
IBI\_Iddir=IOU & 0.00 &  0.00 & 0.00 \\
IBI\_Iddir=IOU\_C & 0.00 & 0.17 & 0.80 \\
IBI\_Iddir=IOU\_Iddir & 0.00 & 0.00 & 0.00 \\
IBI\_Iddir=IOU\_Iddir\_C & 0.00 & 0.00 & 0.00 \\
IOU=IOU\_C & 0.00 & 0.00 & 0.00 \\
IOU=IOU\_Iddir & 0.00 & 0.00 & 0.00 \\
IOU=IOU\_Iddir\_C & 0.00 & 0.00 & 0.00 \\
IOU\_C=IOU\_Iddir & 0.00 & 0.00 & 0.00 \\
IOU\_C=IOU\_Iddir\_C & 0.00 & 0.00 & 0.00 \\
IOU\_Iddir=IOU\_Iddir\_C & 0.00 & 0.00 & 0.00 \\
\hline
\end{tabular}
\begin{tabular}{p{0.8\textwidth}}
Notes: Robust standard errors in parentheses adjusted for 144 clusters at the \textit{Iddir} level. \textit{Kebele} fixed effects capture 12 \textit{Kebele} (municipalities) across 3 Districts. Additional controls include Age, Male, Married, Education level, Family size, Income last month, Drought dummy, Insured before dummy, Maize production, Haricot production, Teff production, Sorghum production, Wheat production, Barley production, Land size, and Savings. ***p $<$ 0.01, **p $<$ 0.05, *p $<$ 0.1. Wald tests show p-values of equality.
\end{tabular}
\end{table}


\section{Extension}

This paper not only replicates the study by Belissa et al. (2019) on the adoption of weather insurance by smallholder farmers in Ethiopia, but also extends it by examining heterogeneous treatment effects using a Multi-Arm-Causal-Forest model. The original study found that the adoption of drought insurance among smallholder farmers could be increased by introducing a novel insurance product that delayed repayment until after harvest to alleviate liquidity constraints. The authors made three modifications to the product to increase uptake, and our extension aims to examine whether financial constraints indeed limit uptake, particularly among poorer smallholders. Our analysis focuses on household income and savings, using machine learning to estimate treatment effects across several treatments. Note our treatment data suffers from errors, so we are unlikely to find significant results. The study's findings have implications for policymakers and stakeholders working to improve the financial resilience of smallholder farmers in developing countries and increase their investment in new technologies. This extension builds on previous research and provides a more nuanced understanding of the factors that affect insurance uptake among smallholder farmers in developing countries.

While our primary variables of interest are income and savings, we take a data-driven approach to identify which variables in the dataset are most pertinent to insurance uptake. To accomplish this, we train a multi-arm causal-forest model using all twenty variables listed in the original paper's balancing tables. We also include information on treatment (randomisation) and outcomes (uptake) Our analysis reveals that income is the most frequently used variable to split the data in the model, followed by several farm output quantities such as wheat, teff, and maize. Interestingly, we find that haricot, sorghum, and barley productions are not considered relevant. In addition to income, only education, cultivated land size, and the savings dummy variable are used in more than 5% of cases.

\begin{table}
\centering
\begin{tabular}{l|r}
\hline
  & x\\
\hline
TincomelastMnth & 0.2269768\\
\hline
Wheatqty & 0.1655113\\
\hline
Teffqty & 0.1388438\\
\hline
maizeqty & 0.1356723\\
\hline
Education & 0.0650387\\
\hline
Cultlandsize10\_a & 0.0633105\\
\hline
saving\_dummy & 0.0516264\\
\hline
sex\_dummy & 0.0467304\\
\hline
FSevdrought & 0.0451323\\
\hline
Age & 0.0423752\\
\hline
Famsize & 0.0171154\\
\hline
buyIBIdummy & 0.0010966\\
\hline
HaricotQty & 0.0004596\\
\hline
marriage\_dummy & 0.0000943\\
\hline
SorghumQty & 0.0000118\\
\hline
Barelyqty & 0.0000047\\
\hline
\end{tabular}
\end{table}

In our initial attempt, we eliminated observation variables with a variable importance score below 5%. However, we encountered overfitting issues due to the high variability of the data and the limited number of observations (8,759).

A considerable amount of the highly variable data has numerous zero observations, making the use of quintiles or median splits unfeasible. Consequently, we chose to discard production variables, despite their prominent use in splits, and focused instead on more easily categorical data, such as age, education, marital status, log income, and savings, with the objective of avoiding overfitting. Although income has a large number of zero observations, we retained it to examine whether financially constrained smallholders derive more benefits from IOU treatments.

\begin{table}
\centering
\begin{tabular}{l|r}
\hline
  & x\\
\hline
LTincomelastMnth & 0.4499650\\
\hline
Age & 0.1791593\\
\hline
Education & 0.1703164\\
\hline
saving\_dummy & 0.1259302\\
\hline
Famsize & 0.0720186\\
\hline
marriage\_dummy & 0.0026106\\
\hline
\end{tabular}
\end{table}

Running the model with our revised set of covariates we find that the the multi-arm causal model struggles to estimate propensity scores for our data. Propensity scores in the data are close to zero. Our model outputs are therefore unlikely to be reliable. We have identified several reasons why this may be the case. (1) High Dimensional Covariates, (2) Imbalanced Treatment Assignment, (3)  Model mispecification (4) Non-linear relationships between covariates and treatment assignment.

Guo and Fraser (2015) suggest that high dimensional covariates can lead to extreme propensity scores, potentially caused by overfitting. To mitigate this issue, we excluded output quantities and focused on binary variables. However, we retained Income, a highly dimensional variable. According to Wager and Athey (2018), imbalanced treatment arms may result from differences in the size of treatment groups, as seen in our first table. Additionally, non-randomized control groups may introduce bias in propensity score estimation, as noted by Rosenbaum and Rubin (1983). Finally, non-linear relationships between covariates and treatment assignment can also impact propensity score estimation, according to Pearl (2016). While we primarily use logged or binary data in our estimation, this remains a possibility to consider.

Despite our concerns about potential estimation issues, we proceeded to plot conditional average treatment effects (CATE) against log income for all treatment arms. Surprisingly, we found that CATE increases with income in all cases, contrary to the original paper's findings. This suggests that financial constraints may not be the main driver of insurance uptake. However, we cannot rule out the possibility that our income measure is unreliable. Many households reported no income in the previous month, which could be due to their reliance on agricultural incomes from harvests or operating in barter economies. Unfortunately, we cannot report confidence intervals as the variance estimates are extremely low, which could be misleading in terms of our level of confidence.

![Figure 4: Income by CATE.](2_Analysis//C_Output//Income_plots.png)

We observe a comparable trend in our savings data, where the conditional average treatment effects (CATE) rise as savings increase. Furthermore, we notice a greater response of CATE to income and savings when iddir leaders are part of the marketing channel. It is plausible that iddir involvement is more effective in cases where smallholders are not financially constrained, and where their hesitation to buy drought-based insurance is due to a lack of confidence or knowledge about the product.

![Figure 5: Savings by CATE.](2_Analysis/C_Output/savings_plots.png)

We calculate ATE estimates for each treatment. We see that IOU and Iddir treatments increase insurance uptake. However the addition of more strict contracts, has a negative affect on uptake. These results are not statistically significant. However they are consistent with the authors original findings. 

\begin{table}
\centering
\begin{tabular}{l|r|r|l|l}
\hline
  & estimate & std.err & contrast & outcome\\
\hline
Standard Iddir - Standrd & 0.0930619 & 0.0718894 & Standard Iddir - Standrd & Y.1\\
\hline
IOU Iddir Contract - Standrd & 0.2734636 & 0.1144630 & IOU Iddir Contract - Standrd & Y.1\\
\hline
IOU Iddir - Standrd & 0.3279834 & 0.1162710 & IOU Iddir - Standrd & Y.1\\
\hline
IOU Contract - Standrd & 0.0890267 & 0.0732976 & IOU Contract - Standrd & Y.1\\
\hline
IOU - Standrd & 0.3182769 & 0.1641750 & IOU - Standrd & Y.1\\
\hline
\end{tabular}
\end{table}

Regrettably, due to constraints in the coding, we cannot generate diverse treatment effects for the multi-arm causal model. The grf package does not support heterogeneous treatment effects in the causal model framework. Hence, we aim to present some partial evidence on the issue by simplifying the problem. As mentioned previously, it seems that the iddir marketing channel has a favorable effect on drought-based insurance uptake. To investigate whether Iddir's influence has an unequal impact on relatively more affluent landowners, we split our sample into those who were treated through the iddir marketing channel, regardless of the contract.



\section{Conclusion}

In this study, we replicate the main results obtained by Belissa et al. (2019) and use the causal forests developed by Waiger and Athey (2017) to conduct heterogeneity analysis, trying to have a closer look at the heterogeneous effects of the multi-arm treatments of the insurance design on households with different socio-economic and production characteristics.

The replication task does not fully reproduce the results obtained by the authors. We download the dataset from the official website and find an obvious data issue that leads to discrepancies in the results---the numbers of \textit{Iddirs} of three treatment groups are much higher than those shown in the original paper. This indicates that some \textit{Iddirs} received different types of policy impacts, which is different from what the authors state in the paper. With inability to solve this issue, we show that our randomisation does not work as well as the original study. However, we still obtain very similar results in the subsequent analysis, including the uptake rates of different types of insurance designs and the effects of the insurance designs gained from the regressions. We can still conclude that delaying weather insurance payment (IOU) increases uptake and promoting weather insurance via \textit{Iddir} leaders increases uptake of IOU. And we also find a negative role of binding contracts.

To summarize, our analysis sheds light on the factors that affect the uptake of drought-based insurance in rural Ethiopia. Our results suggest that financial constraints alone may not be the only obstacle to insurance uptake, as modifying insurance could particularly benefit individuals with higher incomes. However, we must exercise caution in interpreting these findings, given the potential for overfitting or data quality issues. Overall, our study underscores the potential advantages of leveraging causal inference techniques to explore complex issues in development economics, and offers insights for devising more effective insurance schemes in rural settings.

Furthermore, we will present our discoveries on the impact of the iddir marketing channel on individuals with higher incomes and savings, as gauged by the single-arm causal forest analysis. Our visual findings imply that iddir promotions are more likely to entice individuals with higher incomes to participate, which has significant implications for the development of marketing strategies for insurance programs in rural Ethiopia. We will also discuss the limitations of our study, such as data quality concerns and potential overfitting, and suggest avenues for future research in this field.

\bibliographystyle{aea}
\bibliography{references}

% The appendix command is issued once, prior to all appendices, if any.
\appendix

\section{Coding Appendix}
## Script Structure ###########################################################

  # A. Replication 
  # B. Extension 
## A. Replication #############################################################

  # 1. Library Packages
  # 2. Prepare Data
  # 3. Check Structure..
  # 4. Randomisation
  # 5. Balance Test 1
  # 6. Balance Test 2
  # 7. Insurance Uptake Rate
  # 8. Default Rates
  # 9. Plots and Graphs

## 1. Library Packages ########################################################

library(tidyverse) # Data Manipulation Package
library(stargazer) # Latex Tables Packages
library(grf) # Fitting Causal Forest Package
library(caret) # Machine Learning Package
library(sandwich) # Standard Error Adjustment Package
library(lmtest) # Regression Model Testing Package
library(kableExtra) # Create LaTeX tables
library(broom) # Convert statistical models into data frames
library(cobalt)
library(gridExtra)

options(scipen = 999)
## 2. Prepare Data ############################################################

# Load Data
data <- readRDS(file.path("1_Build//C_Output//Cleaned_Data.rds"))


# Copy Data to Inputs
write_csv(data, "2_Analysis//A_Input//Cleaned_Data.csv")

## 3. Check Structure #########################################################

# Consider moving formatting to Build file to tidy up analysis. 

# Check Data Structure
str(data)

# Convert Date
data <- data %>% 
  mutate(IntervDate = as.Date(IntervDate, format = "%B %d, %Y"),
         Dateinterview2 = as.Date(Dateinterview2, format = "%B %d, %Y"))

# Convert Factors
data <- data %>% 
  mutate(across(where(is.character),as_factor))

levels(data$Randomization1)


# Specify "Standard Insurance (coops)" as the reference level (=0)
data$Randomization1 <- factor(data$Randomization1, levels = c("standard insurance through the usual channel (coops)", 
                                                              "Standard insurance through iddirs", 
                                                              "IOU insurance through iddirs with BC",
                                                              "IOU insurance through iddirs without BC",
                                                              "IOU insurance through the usual channel with BC",
                                                              "IOU insurance through usual channel without BC"))

# Specify factors levels to match Randomization
data$Uptake1 <- factor(data$Uptake1, levels = c("standard insurance through the usual channel (coops)", 
                                                              "Standard insurance through iddirs", 
                                                              "IOU insurance through iddirs with BC",
                                                              "IOU insurance through iddirs without BC",
                                                              "IOU insurance through the usual channel (coops) with BC",
                                                              "IOU insurance through usual channel (coops) without BC",
                                                              "Non-buyer"))
# Adjust names to match Randomization 1
levels(data$Uptake1) <- c("standard insurance through the usual channel (coops)",
                          "Standard insurance through iddirs", 
                          "IOU insurance through iddirs with BC",
                          "IOU insurance through iddirs without BC",
                          "IOU insurance through the usual channel with BC",
                          "IOU insurance through usual channel without BC",
                          "Non-buyer")

levels(data$Randomization1)

# Dummy Gender
data$sex_dummy <- ifelse(data$Sex == "Male", 1,0)

# Dummy Marriage
data$marriage_dummy <- ifelse(data$Mstatus == "Married", 1,0)

# Dummy Saving
data$saving_dummy <- ifelse(data$HaveSaving12_a == "Yes", 1,0)

# Log TIncomeLastMnth
data$LTincomelastMnth <- log(data$TincomelastMnth+1)

# Numeric Education
data$Education <- as.numeric(as.character(data$Education))

# Numeric Farm Size
data$Famsize <- as.numeric(as.character(data$Famsize))

# Define Income Quintiles
thirds <- quantile(data$LTincomelastMnth, probs = seq(0, 1, 0.5), duplicates.ok = TRUE)


# create a new column indicating the quintile of each observation
data$Incm_mdn <- cut(data$LTincomelastMnth, breaks = thirds, labels = FALSE, duplicates.ok = TRUE)

quantile <- quantile(data$Cultlandsize10_a, probs = seq(0, 1, 0.2), duplicates.ok = TRUE)

data$clt_lnd_qntl <- cut(data$Cultlandsize10_a, breaks = quantile , labels = FALSE, duplicates.ok = TRUE)

plot(data$Cultlandsize10_a)
# Create Treatment Dummies  "Standard Insurance (coops)" set as 0
treatment_dummy <- model.matrix(~ Randomization1 - 1, data = data)

levels(data$Uptake1)

uptake_dummy <- model.matrix(~ Uptake1 - 1, data = data)

# Combine Data and Dummies
data <- cbind(data, treatment_dummy)
data <- cbind(data, uptake_dummy)

# Rename Randomization Dummies
data <- data %>%
  rename(Dum_Insrnce_Stndrd = "Randomization1standard insurance through the usual channel (coops)", 
         Dum_Insrnce_Iddr = "Randomization1Standard insurance through iddirs",
         Dum_IOU_Iddr_BC = "Randomization1IOU insurance through iddirs with BC",
         Dum_IOU_Iddr = "Randomization1IOU insurance through iddirs without BC",
         Dum_IOU_BC = "Randomization1IOU insurance through the usual channel with BC",
         Dum_IOU = "Randomization1IOU insurance through usual channel without BC")

# Rename Treatment Dummies
data <- data %>%
  rename(Dum_Trt_Insrnce_Stndrd = "Uptake1standard insurance through the usual channel (coops)", 
         Dum_Trt_Insrnce_Iddr = "Uptake1Standard insurance through iddirs",
         Dum_Trt_IOU_Iddr_BC = "Uptake1IOU insurance through iddirs with BC",
         Dum_Trt_IOU_Iddr = "Uptake1IOU insurance through iddirs without BC",
         Dum_Trt_IOU_BC = "Uptake1IOU insurance through the usual channel with BC",
         Dum_Trt_IOU = "Uptake1IOU insurance through usual channel without BC",
         Dum_Trt_NB = "Uptake1Non-buyer")

# Summary Statistics
treatment_count <- data %>% 
  group_by(Randomization1) %>%
  summarise(Observations = n_distinct(Identifier),
            Iddir = n_distinct(iddir)) 

## 4.Randomisation ############################################################

# To Do:
  # Replication Problem 1: Iddir numbers per category do not match with the Section 3 Randomization
    # Observation numbers correct
    # Iddir and iddir are off slightly. Should match exactly
    # Randomization should align with Iddir level. 1 per Iddir not the case
    # Checked analysis script not jumbling factor levels. 
    # Checked cleaning script nothing jumbling the data
    # Checked input data, problem is in the dataset
    # Downloaded data again, problem persists. 
    # Downloaded data from different source. Same problem
    # Contacted author to see if we can get a clean copy

# Table Results

  # Treatment Summary Statistics - Correct
treatment_count <- data %>% 
  group_by(Randomization1) %>%
  summarise(Observations = n_distinct(Identifier),
            Iddir = n_distinct(Iddir))

  # Check distinct observations by column to try find alternatives catrgorisatioon
data %>% summarize(across(everything(), n_distinct))

  # Uptake observations too low, Iddir numbers closer to true
treatment_count <- data %>% 
  group_by(Uptake1) %>%
  summarise(Observations = n_distinct(Identifier),
            Iddir = n_distinct(iddir))


## 5. Balance Test 1 ##########################################################

# To Do:
# Wald Tests
# Table Results

# Balance Test 1a 

  # List y variable for purr
outcomes <- c("Age", "sex_dummy", "marriage_dummy", "Education", "Famsize", "TincomelastMnth", "FSevdrought", "buyIBIdummy")

  # List covariates for purr
covariates <- c("Dum_Insrnce_Iddr", "Dum_IOU_Iddr_BC", "Dum_IOU_Iddr", "Dum_IOU_BC", "Dum_IOU")

wald_combinations <- list(c("Dum_Insrnce_Iddr", "Dum_IOU"),
                          c("Dum_Insrnce_Iddr", "Dum_IOU_BC"),
                          c("Dum_Insrnce_Iddr", "Dum_IOU_Iddr"),
                          c("Dum_Insrnce_Iddr", "Dum_IOU_Iddr_BC"),
                          c("Dum_IOU", "Dum_IOU_BC"),
                          c("Dum_IOU", "Dum_IOU_Iddr"),
                          c("Dum_IOU", "Dum_IOU_Iddr_BC"),
                          c("Dum_IOU_BC", "Dum_IOU_Iddr"),
                          c("Dum_IOU_BC", "Dum_IOU_Iddr_BC"),
                          c("Dum_IOU_Iddr", "Dum_IOU_Iddr_BC"))

  # Create balancing table list
balancing_test_1a <- list()

  # Create Wald test list
wald_test_1a <- list()

# Create an empty list to store the model summaries
model_summaries <- list()

# Loop through the outcome variable variables using purrr and map regression result to list 
balancing_test_1a <- map(outcomes, function(outcomes) {
  # Define your formula
  formula <- paste(outcomes, "~", paste(covariates, collapse = "+"))
  
  # Run the regression model
  model <- lm(formula, data = data)

  
  # Loop through wald combinations using purr and map wald results to list
  wald_test_1a <- map(wald_combinations, function(wald_combinations) {
    
    # Wald test
    wald_test <- waldtest(model, vcov = vcovHC(model, cluster = "iddir"), paste(wald_combinations))
    
    # Extract F-value
    w_coefficients <- na.omit(wald_test$"Pr(>F)")
    
    # Save coefficients as dataframe
    w_coefficients_df <- as.data.frame(w_coefficients)

    # Save Wald combinations as column   
    w_coefficients_df$wald_combinations <- list(paste(wald_combinations, collapse = "_"))
    
    # Save Column reference as column
    w_coefficients_df$outcomes <- outcomes
    
    return(w_coefficients_df)                                  
  })
  
  # Save the wald test result for this outcome in wald_test_1a list
  w_results_df <- bind_rows(wald_test_1a)
  
  # Extract the summary of the model
  model_summary <- summary(model)
  
  # Extract the coefficients from the summary
  coefficients <- model_summary$coefficients
  
  # Add the coefficients to a data frame
  coefficients_df <- as.data.frame(coefficients)
  
  # Add the y variable name to the data frame
  coefficients_df$outcomes <- outcomes
  
  # Return the data frames
  list(model_summary = model_summary, w_results_df = w_results_df)
})

  # Combine the model summaries and wald test results into separate lists
model_summaries_1a <- map(balancing_test_1a, function(x) x$model_summary)
w_results_list_1a <- map(balancing_test_1a, function(x) x$w_results_df)

  # Combine the wald test results into a single data frame
model_1a <- bind_rows(lapply(model_summaries_1a, tidy), .id= "model.id")
w_results_1a <- bind_rows(w_results_list_1a)


  #Clean up redundant objects
rm(model_summaries_1a,model_summaries, wald_test_1a, w_results_list_1a)

head(w_results_1a)

## Balancing Tables

# Balancing Table 1a
table_1aa <- model_1a %>%
  select(-c(std.error, statistic)) %>%
  pivot_longer(cols = c(estimate, p.value), names_to = "variable", values_to = "value") %>%
  pivot_wider(names_from = "model.id", values_from = "value") %>%
  mutate_at(vars(-c(term, variable)), ~round(., digits = 3)) %>% 
  rename_with(~outcomes, 3:10)
 
# Balancing Table 1b
table_1ab <- w_results_1a %>%
  select(wald_combinations, everything()) %>% 
  pivot_wider(names_from = outcomes, values_from = w_coefficients) %>% 
  select(-wald_combinations, starts_with("coefficients_")) %>% 
  round(2)

## 6. Balance Test 1b ##########################################################

# To Do:
# Wald Tests
# Table Results

# Balance Test 1b
colnames(data)

# List of outcome variables for purr
outcomes <- c("maizeqty", "HaricotQty", "Teffqty", "SorghumQty", "Wheatqty", "Barelyqty", "Cultlandsize10_a", "saving_dummy")

# Loop through the list of outcomes and convert corresponding columns in df to numeric
data[outcomes] <- map_dfc(data[outcomes], as.numeric)

# Create balancing table list
balancing_test_1a <- list()

# Create Wald test list
wald_test_1a <- list()

# Create an empty list to store the model summaries
model_summaries <- list()

# Loop through the outcome variable variables using purrr and map regression result to list 
balancing_test_1b <- map(outcomes, function(outcomes) {
  # Define your formula
  formula <- paste(outcomes, "~", paste(covariates, collapse = "+"))
  
  # Run the regression model
  model <- lm(formula, data = data)
  
  
  # Loop through wald combinations using purr and map wald results to list
  wald_test_1b <- map(wald_combinations, function(wald_combinations) {
    
    # Wald test
    wald_test <- waldtest(model, vcov = vcovHC(model, cluster = "iddir"), paste(wald_combinations))
    
    # Extract F-value
    w_coefficients <- na.omit(wald_test$"Pr(>F)")
    
    # Save coefficients as dataframe
    w_coefficients_df <- as.data.frame(w_coefficients)
    
    # Save Wald combinations as column   
    w_coefficients_df$wald_combinations <- list(paste(wald_combinations, collapse = "_"))
    
    # Save Column reference as column
    w_coefficients_df$outcomes <- outcomes
    
    return(w_coefficients_df)                                  
  })
  
  # Save the wald test result for this outcome in wald_test_1b list
  w_results_df <- bind_rows(wald_test_1b)
  
  # Extract the summary of the model
  model_summary <- summary(model)
  
  # Extract the coefficients from the summary
  coefficients <- model_summary$coefficients
  
  # Add the coefficients to a data frame
  coefficients_df <- as.data.frame(coefficients)
  
  # Add the y variable name to the data frame
  coefficients_df$outcomes <- outcomes
  
  # Return the data frames
  list(model_summary = model_summary, w_results_df = w_results_df)
})

# Combine the model summaries and wald test results into separate lists
model_summaries_1b <- map(balancing_test_1b, function(x) x$model_summary)
w_results_list_1b <- map(balancing_test_1b, function(x) x$w_results_df)

# Combine the wald test results into a single data frame
model_1b <- bind_rows(lapply(model_summaries_1b, tidy), .id= "model.id")
w_results_1b <- bind_rows(w_results_list_1b)


#Clean up redundant objects
rm(model_summaries_1b,model_summaries, wald_test_1b, w_results_list_1b)

## Balancing Tables

  # Balancing Table 2a
table_1ba <- model_1b %>%
  select(-c(std.error, statistic)) %>%
  pivot_longer(cols = c(estimate, p.value), names_to = "variable", values_to = "value") %>%
  pivot_wider(names_from = "model.id", values_from = "value") %>%
  mutate_at(vars(-c(term, variable)), ~round(., digits = 3)) %>% 
  rename_with(~outcomes, 3:10)

  # Balancing Table 2b
table_1bb <- w_results_1b %>%
  select(wald_combinations, everything()) %>% 
  pivot_wider(names_from = outcomes, values_from = w_coefficients) %>% 
  select(-wald_combinations, starts_with("coefficients_")) %>% 
  round(2)


## 7. Insurance Uptake Rates ##################################################


  # Uptake regressions
uptk_rt_1 <- lm(Dum_Trt_Insrnce_Stndrd ~ Dum_Insrnce_Stndrd, data)
uptk_rt_2 <- lm(Dum_Trt_Insrnce_Iddr ~ Dum_Insrnce_Iddr, data)
uptk_rt_3 <- lm(Dum_Trt_IOU_Iddr_BC ~ Dum_IOU_Iddr_BC, data)
uptk_rt_4 <- lm(Dum_Trt_IOU_Iddr ~ Dum_IOU_Iddr, data)
uptk_rt_5 <- lm(Dum_Trt_IOU_BC ~ Dum_IOU_BC, data)
uptk_rt_6 <- lm(Dum_Trt_IOU ~ Dum_IOU, data)

# Compute robust clustered errors
uptk_rt_c1 <- vcovHC(uptk_rt_1, cluster="Iddir")
uptk_rt_c2 <- vcovHC(uptk_rt_2, cluster="Iddir")
uptk_rt_c3 <- vcovHC(uptk_rt_3, cluster="Iddir")
uptk_rt_c4 <- vcovHC(uptk_rt_4, cluster="Iddir")
uptk_rt_c5 <- vcovHC(uptk_rt_5, cluster="Iddir")
uptk_rt_c6 <- vcovHC(uptk_rt_6, cluster="Iddir")

# Extract coefficient estimates and confidence intervals for the first coefficient of each model
uptk_rt_1a <- confint(coeftest(uptk_rt_1, uptk_rt_c1))[2,]
uptk_rt_2a <- confint(coeftest(uptk_rt_2, uptk_rt_c2))[2,]
uptk_rt_3a <- confint(coeftest(uptk_rt_3, uptk_rt_c3))[2,]
uptk_rt_4a <- confint(coeftest(uptk_rt_4, uptk_rt_c4))[2,]
uptk_rt_5a <- confint(coeftest(uptk_rt_5, uptk_rt_c5))[2,]
uptk_rt_6a <- confint(coeftest(uptk_rt_6, uptk_rt_c6))[2,]

# Create a data frame with coefficient estimates and confidence intervals for each regression model
figure_2_df <- data.frame(Model=c("Index\nInsurance", "Index Insurance\nvia Iddir", "IOU via Iddir\nwith Contract", "IOU via\nIddir", "IOU with\nContract", "IOU"),
                 Estimate=c(coef(uptk_rt_1)[2], coef(uptk_rt_2)[2], coef(uptk_rt_3)[2], coef(uptk_rt_4)[2], coef(uptk_rt_5)[2], coef(uptk_rt_6)[2]),
                 LowerCI=c(uptk_rt_1a[2], uptk_rt_2a[2], uptk_rt_3a[2], uptk_rt_4a[2], uptk_rt_5a[2], uptk_rt_6a[2]),
                 UpperCI=c(uptk_rt_1a[1], uptk_rt_2a[1], uptk_rt_3a[1], uptk_rt_4a[1], uptk_rt_5a[1], uptk_rt_6a[1]))

# Plot bar graph with error bars
figure_2 <- ggplot(figure_2_df, aes(x=Model, y=Estimate)) +
  geom_bar(stat="identity", fill="gray50") +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI), width=.2, position=position_dodge(.9)) +
  labs(x="Randomisation", y="Coefficient Estimate (%)", 
       title="Mean of the First Coefficient with 95% Confidence Intervals") + 
  theme_classic() 

  # Cross-Tabulation of Treatment-uptake. Compilers on diagonal 
cross_tab <- data %>% 
  select(Randomization1, Uptake1) %>% 
  count(Randomization1, Uptake1) %>% 
  pivot_wider(names_from =Randomization1, values_from = n, values_fill= 0) %>%
  arrange(Uptake1)

# Identify common levels between Randomization1 and Uptake1
common_levels <- intersect(levels(data$Randomization1), levels(data$Uptake1))

data %>% 
  select(Randomization1, Uptake1, Iddir) %>% 
  group_by(Randomization1) %>%
  summarise(match_pct = sum(as.character(Randomization1) == as.character(Uptake1)) / n() * 100) %>%
  ungroup() 

# Models
  # Parsimonious Model 
prsmns_mdl  <- lm(Uptake1dummy ~ Dum_Insrnce_Iddr + Dum_IOU_Iddr_BC + Dum_IOU_Iddr + Dum_IOU_BC + Dum_IOU, data = data)
prsmns_se <- vcovCL(prsmns_mdl, cluster = data$Iddir)
prsmns_mdl_2 <- coeftest(prsmns_mdl, prsmns_se)

  # Additional Model 
addtnl_mdl  <- lm(Uptake1dummy ~ Dum_Insrnce_Iddr + Dum_IOU_Iddr_BC + Dum_IOU_Iddr + Dum_IOU_BC + Dum_IOU + Age + sex_dummy + marriage_dummy + Education + Famsize + TincomelastMnth + FSevdrought + buyIBIdummy + maizeqty + HaricotQty + Teffqty + SorghumQty + Wheatqty + Barelyqty + Cultlandsize10_a + saving_dummy + factor(Kebele), data = data)
addtnl_se <- vcovCL(addtnl_mdl, cluster = data$Iddir)
addtnl_mdl_2  <- coeftest(addtnl_mdl, addtnl_se)

  # Excluding Daloti #Mati
data_filtered <- data %>% filter(Kebele != "Dalota Mati")
excl_DltMt_mdl <- lm(Uptake1dummy ~ Dum_Insrnce_Iddr + Dum_IOU_Iddr_BC + Dum_IOU_Iddr + Dum_IOU_BC + Dum_IOU + Age + sex_dummy + marriage_dummy + Education + Famsize + TincomelastMnth + FSevdrought + buyIBIdummy + maizeqty + HaricotQty + Teffqty + SorghumQty + Wheatqty + Barelyqty + Cultlandsize10_a + saving_dummy + factor(Kebele), data = data_filtered)
excl_DltMt_se <-vcovCL(excl_DltMt_mdl, cluster = data_filtered$Iddir)
excl_DltMt_mdl_2  <- coeftest(excl_DltMt_mdl, excl_DltMt_se)


# Wald Tests Model prsmns_mdl
mdl_1_w1 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU"))
mdl_1_w2 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU_BC"))
mdl_1_w3 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU_Iddr"))
mdl_1_w4 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU_Iddr_BC"))
mdl_1_w5 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_IOU", "Dum_IOU_BC"))
mdl_1_w6 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_IOU", "Dum_IOU_Iddr"))
mdl_1_w7 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_IOU", "Dum_IOU_Iddr_BC"))
mdl_1_w8 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_IOU_BC", "Dum_IOU_Iddr"))
mdl_1_w9 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_IOU_BC", "Dum_IOU_Iddr_BC"))
mdl_1_w10 <- waldtest(prsmns_mdl, vcov = vcovHC(prsmns_mdl, cluster = "Iddir"), c("Dum_IOU_Iddr", "Dum_IOU_Iddr_BC"))

# Wald Tests Model addtnl_mdl
md2_1_w1 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU"))
md2_1_w2 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU_BC"))
md2_1_w3 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU_Iddr"))
md2_1_w4 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU_Iddr_BC"))
md2_1_w5 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_IOU", "Dum_IOU_BC"))
md2_1_w6 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_IOU", "Dum_IOU_Iddr"))
md2_1_w7 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_IOU", "Dum_IOU_Iddr_BC"))
md2_1_w8 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_IOU_BC", "Dum_IOU_Iddr"))
md2_1_w9 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_IOU_BC", "Dum_IOU_Iddr_BC"))
md2_1_w10 <- waldtest(addtnl_mdl, vcov = vcovHC(addtnl_mdl, cluster = "Iddir"), c("Dum_IOU_Iddr", "Dum_IOU_Iddr_BC"))

# Wald Tests Model excl_DltMt_mdl
md3_1_w1 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU"))
md3_1_w2 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU_BC"))
md3_1_w3 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU_Iddr"))
md3_1_w4 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_Insrnce_Iddr", "Dum_IOU_Iddr_BC"))
md3_1_w5 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_IOU", "Dum_IOU_BC"))
md3_1_w6 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_IOU", "Dum_IOU_Iddr"))
md3_1_w7 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_IOU", "Dum_IOU_Iddr_BC"))
md3_1_w8 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_IOU_BC", "Dum_IOU_Iddr"))
md3_1_w9 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_IOU_BC", "Dum_IOU_Iddr_BC"))
md3_1_w10 <- waldtest(excl_DltMt_mdl, vcov = vcovHC(excl_DltMt_mdl, cluster = "Iddir"), c("Dum_IOU_Iddr", "Dum_IOU_Iddr_BC"))


## 8. Default Uptake Rates ####################################################

## 9. Plots and Graphs ########################################################

# Table 0 Observation and Iddirs
kable(treatment_count, "latex") %>%
  kable_styling(bootstrap_options = NULL)

# Table 1a Balancing Test 1: Regressions 
table_1aa
kable(table_1aa, "latex") %>%
  kable_styling(bootstrap_options = NULL)

# Table 1b Balancing Test 1: Wald
table_1ab
kable(table_1ab, "latex") %>%
  kable_styling(bootstrap_options = NULL)

# Table 2a Balancing Test 2: Regressions 
table_1ba
kable(table_1ba, "latex") %>%
  kable_styling(bootstrap_options = NULL)

# Table 2b Balancing Test 2: Wald
table_1bb
kable(table_1bb, "latex") %>%
  kable_styling(bootstrap_options = NULL)

# Additional: Cross-Tabulation of Treatment-uptake. Compliers on diagonal
cross_tab 

# Figure 2

figure_2

ggsave(filename = "figure_2", plot = figure_2, device = "png", path = "2_Analysis//C_Output//")


# Tables

# Probably needs to be composed manually. 




## B. Extension ###############################################################

  # 1. Library Packages
  # 2. Prepare Data
  # 3. Split Data
  # 4. Fit Causal Forest Model
  # 5. Predict Treatment Effects
  # 6. Calculate Average Treatment Effects
  # 7. Evaluate Model
  # 8. Plots and Graphs

## 1. Prepare Data ############################################################

# step 1: prepare run a trial causal forest model with our balancing variables to test which variables are identified as most important for the causal forest algorithm. 
# step 2: 

# Test 


  # Relevant Variables
relevant_columns <- c("Identifier",
                      "Randomization1",
                      "Uptake1dummy",
                      "Dum_Insrnce_Stndrd",
                      "Dum_Insrnce_Iddr",
                      "Dum_IOU_Iddr_BC",
                      "Dum_IOU_Iddr",
                      "Dum_IOU_BC",
                      "Dum_IOU",
                      "Age", 
                      "sex_dummy", 
                      "marriage_dummy", 
                      "Education", 
                      "Famsize", 
                      "TincomelastMnth", 
                      "FSevdrought", 
                      "buyIBIdummy",
                      "maizeqty", 
                      "HaricotQty", 
                      "Teffqty", 
                      "SorghumQty", 
                      "Wheatqty", 
                      "Barelyqty", 
                      "Cultlandsize10_a", 
                      "saving_dummy",
                      "Iddir") 

cf_data <- data %>% select(relevant_columns)
  # Separate Dataset
levels(cf_data$Randomization1) <- c("Standrd", "Standard Iddir","IOU Iddir Contract", "IOU Iddir", "IOU Contract", "IOU")

  # Format Outcome
outcome <- as.matrix(cf_data$Uptake1)

treatment <- as.factor(cf_data$Randomization1)

# Format Covariates
covariates <- as.matrix(data[,c("Age", 
                                "sex_dummy", 
                                "marriage_dummy", 
                                "Education", 
                                "Famsize", 
                                "TincomelastMnth", 
                                "FSevdrought", 
                                "buyIBIdummy",
                                "maizeqty", 
                                "HaricotQty", 
                                "Teffqty", 
                                "SorghumQty", 
                                "Wheatqty", 
                                "Barelyqty", 
                                "Cultlandsize10_a", 
                                "saving_dummy")])


covariate_names <- c("Age", 
                     "sex_dummy", 
                     "marriage_dummy", 
                     "Education", 
                     "Famsize", 
                     "TincomelastMnth", 
                     "FSevdrought", 
                     "buyIBIdummy",
                     "maizeqty", 
                     "HaricotQty", 
                     "Teffqty", 
                     "SorghumQty", 
                     "Wheatqty", 
                     "Barelyqty", 
                     "Cultlandsize10_a", 
                     "saving_dummy")

# Format Iddir Cluster as numeric
cf_data$Iddir <- as.numeric(cf_data$Iddir)

# Causal Forest Main Attempt
multi_cf_test <- multi_arm_causal_forest(covariates, #Covariates
                                      outcome, # Outcomes
                                      treatment, #Treatments
                                      cluster= cf_data$Iddir, # cluster error by Iddir
                                      num.trees = 2000, # standard tree number
                                      honesty = TRUE, #Honesty on 
                                      seed =123) # reproducible seed

# Split by covariate
multi_cf_1_vi_t <- c(variable_importance(multi_cf_test))

# Names covariate importance
names(multi_cf_1_vi_t) <- covariate_names

# Sort covariates by importance
multi_cf_1_vi_t <- sort(multi_cf_1_vi_t, decreasing = TRUE)

# Print importance
print(multi_cf_1_vi_t)






## 2. Fit Model  ##############################################################

# create a new column indicating the quintile of each observation
data$Incm_qntl <- cut(data$LTincomelastMnth, breaks = thirds, labels = FALSE, duplicates.ok = TRUE)

quantile <- quantile(data$Cultlandsize10_a, probs = seq(0, 1, 0.2), duplicates.ok = TRUE)

data$clt_lnd_qntl <- cut(data$Cultlandsize10_a, breaks = quantile , labels = FALSE, duplicates.ok = TRUE)


# Relevant Variables
relevant_columns <- c("Identifier",
                      "Randomization1",
                      "Uptake1dummy",
                      "Dum_Insrnce_Stndrd",
                      "Dum_Insrnce_Iddr",
                      "Dum_IOU_Iddr_BC",
                      "Dum_IOU_Iddr",
                      "Dum_IOU_BC",
                      "Dum_IOU",
                      "Age",
                      "Education",
                      "marriage_dummy",
                      "Famsize",
                      "LTincomelastMnth",
                      "saving_dummy",
                      "Iddir") 

cf_data <- data %>% select(relevant_columns)
# Separate Dataset
levels(cf_data$Randomization1) <- c("Standrd", "Standard Iddir","IOU Iddir Contract", "IOU Iddir", "IOU Contract", "IOU")

# Format Outcome
outcome <- as.matrix(cf_data$Uptake1)

treatment <- as.factor(cf_data$Randomization1)

# Format Covariates
covariates <- as.matrix(data[,c("Age",
                                "Education",
                                "marriage_dummy",
                                "Famsize",
                                "LTincomelastMnth",
                                "saving_dummy")])


covariate_names <- c("Age",
                     "Education",
                     "marriage_dummy",
                     "Famsize",
                     "LTincomelastMnth",
                     "saving_dummy")

# Format Iddir Cluster as numeric
cf_data$Iddir <- as.numeric(cf_data$Iddir)


# Causal Forest Main Attempt
multi_cf_1 <- multi_arm_causal_forest(covariates, #Covariates
                                      outcome, # Outcomes
                                      treatment, #Treatments
                                      cluster= cf_data$Iddir, # cluster error by Iddir
                                      num.trees = 2000, # standard tree number
                                      honesty = TRUE, #Honesty on 
                                      seed =123) # reproducible seed
summary(multi_cf_1)

  # Don't look at distribution of predictions. Unreliable. See 4.2.3: https://bookdown.org/stanfordgsbsilab/ml-ci-tutorial/hte-i-binary-treatment.html

# See how often a variable was used to split a tree

  # Split by covariate
multi_cf_1_vi <- c(variable_importance(multi_cf_1))

  # Names covariate importance
names(multi_cf_1_vi) <- covariate_names

  # Sort covariates by importance
multi_cf_1_vi <- sort(multi_cf_1_vi, decreasing = TRUE)

  # Print importance
print(multi_cf_1_vi)

# Get predictions from fitted forest
multi_cf_1_tau <- predict(multi_cf_1, estimate.variance = T)

# Plot CATES. This is a very bad way of coding. However the data table is very messy and I want to preserve the order to allow us to plot against covariates.

  # Pull prediction data 

    # pull raw prediction
predictions <- as.data.frame(multi_cf_1_tau$predictions) 

      # rename treatments for simplicity
colnames(predictions) <- c("Standard Iddir","IOU Iddir Contract", "IOU Iddir", "IOU Contract", "IOU")

      # apply sequence to the data so I can retain correct order for plots
predictions <-  predictions %>% mutate(seq_number = row_number())

      # Pivot data long for merge
predictions <- predictions %>% pivot_longer(cols = c("Standard Iddir","IOU Iddir Contract", "IOU Iddir", "IOU Contract", "IOU"), names_to = "treatment", values_to = "Prediction") 

  # Pull variance data

    # Pull raw variance estimates
variance_est <- as.data.frame(multi_cf_1_tau$variance.estimates) 

    # Rename variance for simplicity
colnames(variance_est) <- c("Standard Iddir","IOU Iddir Contract", "IOU Iddir", "IOU Contract", "IOU") 

    # apply sequence to the data so I can retain correct order for plots
variance_est <-  variance_est %>% mutate(seq_number = row_number()) 

    # Pivot data long for merge
variance_est <- variance_est %>% pivot_longer(cols = c("Standard Iddir","IOU Iddir Contract", "IOU Iddir", "IOU Contract", "IOU"), names_to = "treatment", values_to = "variance") 

  # Merge prediction and variance data.

    # Bind variance to predictions
predictions <- cbind(predictions, variance_est$variance)

    # Rename columns
colnames(predictions) <- cbind("seq_number","treatment", "prediction", "variance") 
head(predictions)

  # Calculate confidence Intervals

    # Calculate the lower and upper confidence intervals for the predictions by group
predictions <- predictions %>%
  group_by(treatment) %>%
  mutate(lower_ci = prediction - 1.96 * sqrt(variance),
         upper_ci = prediction + 1.96 * sqrt(variance)) %>%
  ungroup()

covariates <-  covariates %>% as.data.frame() %>% mutate(seq_number = row_number()) 
predictions <- left_join(predictions, as_tibble(covariates), by = "seq_number")
    
head(predictions)


# Create an empty list to store the plots
Income_plot_list <- list()

for (t in unique(predictions$treatment)) {
  # Subset the data for the current treatment group
  subset_df <- predictions[predictions$treatment == t, ]
  
  # Create the plot using ggplot
  p <- ggplot(subset_df, aes(x = LTincomelastMnth, y = prediction)) +
    geom_point() +
    geom_smooth(method = "lm", se = TRUE, fullrange = TRUE, color = "blue") +
    scale_x_continuous() +
    scale_y_continuous(limits = c(-0.2, .60)) +
    labs(title = paste(t),
         x = "Log Income Previous Month",
         y = "CATE") +
    theme_classic()
  
  # Create a unique name for the plot based on treatment and LTincomelastMnth
  lt_income <- unique(subset_df$LTincomelastMnth)
  plot_name <- paste0(t)
  
  # Create a sequence number to make the file name unique for each iteration
  seq_num <- 1
  while (file.exists(paste0(plot_name, "_", seq_num, ".png"))) {
    seq_num <- seq_num + 1
  }
  plot_name <- paste0(plot_name, "_", seq_num, ".png")
  
  # Save the plot with a unique name
  ggsave(filename = plot_name, plot = p, device = "png", path = "2_Analysis//C_Output//")
  
  # Capture the output of the print function to a variable
  p_output <- capture.output(print(p))
  
  # Assign the plot to a variable name and add it to the plot list
  plot_var_name <- paste0("plot_", t)
  Income_plot_list[[plot_var_name]] <- p
}

# Create an empty list to store the plots
Saving_plot_list <- list()

for (t in unique(predictions$treatment)) {
  # Subset the data for the current treatment group
  subset_df <- predictions[predictions$treatment == t, ]
  
  # Create the plot using ggplot
  p <- ggplot(subset_df, aes(x = LTincomelastMnth, y = prediction)) +
    geom_point() +
    geom_smooth(method = "lm", se = TRUE, fullrange = TRUE, color = "blue") +
    scale_x_continuous() +
    scale_y_continuous(limits = c(-0.2, .60)) +
    labs(title = paste(t),
         x = "Savings",
         y = "CATE") +
    theme_classic()
  
  # Create a unique name for the plot based on treatment and LTincomelastMnth
  lt_income <- unique(subset_df$LTincomelastMnth)
  plot_name <- paste0(t)
  
  # Create a sequence number to make the file name unique for each iteration
  seq_num <- 1
  while (file.exists(paste0(plot_name, "_", seq_num, ".png"))) {
    seq_num <- seq_num + 1
  }
  plot_name <- paste0(plot_name, "_", seq_num, ".png")
  
  # Save the plot with a unique name
  ggsave(filename = plot_name, plot = p, device = "png", path = "2_Analysis//C_Output//")
  
  # Capture the output of the print function to a variable
  p_output <- capture.output(print(p))
  
  # Assign the plot to a variable name and add it to the plot list
  plot_var_name <- paste0("plot_", t)
  Saving_plot_list[[plot_var_name]] <- p
}

# Income CATE plots
Income_plots <- grid.arrange(grobs = Income_plot_list, ncol = 3)

# Savings CATE plots
savings_plots <- grid.arrange(grobs = Saving_plot_list , ncol = 3)

# ATE
multi_ate <-average_treatment_effect(multi_cf_1)


# Calculate Heterogeneous Treatment Effect 

## Old Code Rewriting #####################################################

# Get a warning that the treatment propensities are close to 0 or 1. This means treatment effects are not well identified
  # Issue is present in most treatment groups. minimum treatment propensities in each arm are close to 0.
  # Reasons:
    # (Possible, lots of variables) High Dimensional Covariates: This can lead extreme treatment propensities due to overfitting. Section 3.4 of "Propensity Score Analysis: Statistical Methods and Applications" by Guo and Fraser (2015).
    # (Possible, treatment arms differ significantly in size) Imbalanced Treatment Assignment: Strong bias in treatment where one arm is much larger than the others. Section 2.2 of "Causal Forests: An Alternative to Randomized Controlled Trials" by Wager and Athey (2018)
    # (Possible, ) Model mispecification Section 2.2 of "Propensity Score Methods for Bias Reduction in the Comparison of a Treatment to a Non-Randomized Control Group" by Rosenbaum and Rubin (1983).
    # (Less likely, data is logged or binary) Non-linear relationships between covariates and treatment assignment Section 2.3 of "Causal Inference in Statistics: An Overview" by Pearl (2016).

 
  # Problem solving 1: Reduce Covariates - No solution

# cf_multi indicates that production quantities have a variable importance exceeding .1. However, "maizeqty", "Teffqty", "Wheatqty" exceed 0

# Format Covariates
#covariates <- as.matrix(data[,c("maizeqty",
#                                "HaricotQty",
#                                "Teffqty",
#                                "SorghumQty",
#                                "Wheatqty",
#                                "Barelyqty")])

# Run Multi-Arm Causal Model - Handles Multiple Treatments
#cf_multi <- multi_arm_causal_forest(covariates, outcome, treatment, cluster= cf_data$Iddir,num.trees = 2000, seed =123)

# Potential addition: 
#dbl_rbst_scr <- get_scores(cf_multi) # Improves result but issue persists, low propensity scores. Removing further was not helpful

# Return Datapoints to Fit Model
#cf_predict <- predict(cf_multi)

# Estimate CATE
#cf_predict_CATE <- predict(cf_multi, estimate_cate = TRUE)$predictions

# Problem solving 2: Imbalanced Treatment Assignment, No Solution

  # Format Covariates
#covariates <- as.matrix(data[,c("Age",
#                                "sex_dummy",
#                              "marriage_dummy",
#                                "saving_dummy",
#                                "Education",
#                                "Famsize",
#                                "FSevdrought",
#                                "buyIBIdummy",
#                                "maizeqty",
#                                "HaricotQty",
#                                "Teffqty",
#                                "SorghumQty",
#                                "Wheatqty",
#                                "Barelyqty",
#                                "Cultlandsize10_a")])

  # Check imbalanced treatment assignment
#smds <- cobalt::bal.tab(covariates, treatment, s.d.denom = "standard insurance through the usual channel (coops)")
  
  # Data is imbalanced (exceeds)
# summary(smds$Balance.Across.Pairs) # Max.Diff.Un is very close to the 0.2 threshold, but just below.

# Problem solving 3: Model mispecification. Non randomised control group. Not sure we can resolve this. Data issue

# Problem solving 4: Non linear relationship between control group and covariates. Issue persists despite reduducing no of variables to logged quantities

# Format Covariates
#covariates <- as.matrix(data[,c("LTincomelastMnth")])

# cf_multi <- multi_arm_causal_forest(covariates, outcome, treatment, cluster= cf_data$Iddir,num.trees = 2000, seed =123)

# Potential addition: 
#dbl_rbst_scr <- get_scores(cf_multi)

# Check imbalanced treatment assignment
# smds <- cobalt::bal.tab(covariates, treatment, s.d.denom = "standard insurance through the usual channel (coops)")

## 3. Simple Causal Model  ##########################################################

treatment <- as.matrix(as.numeric(data$xIDDIR))

# Format Covariates
covariates <- as.matrix(data[,c("Incm_mdn",
                                "saving_dummy")])

data <- data %>% mutate(uptake_any= if_else(uptake1gr1 + uptake1gr1 + uptake1gr1 + uptake1gr1 + uptake1gr1 + uptake1gr1 >= 1, 1, 0))

outcome <- as.matrix(data$uptake_any)

# Causal Forest Main Attempt
cf_1 <- causal_forest(covariates, #Covariates
                      outcome, # Outcomes
                      treatment, #Treatments
                      cluster= cf_data$Iddir, # cluster error by Iddir
                      num.trees = 2000, # standard tree number
                      honesty = TRUE, #Honesty on
                      seed =123) # reproducible seed

cf_import <- variable_importance(cf_1)

rowwnames <-  c("Incm_mdn", #important
                  "saving_dummy") #important

cf_plot$tau_hat <- as.data.frame(predict(cf_1, covariates, estimate.variance = T)$predictions)
cf_plot$sigma_hat <- predict(cf_1, covariates, estimate.variance = T)$variance.estimates
cfplot <- cf_plot %>% mutate(upper = tau_hat + 1.96 * sigma_hat,
                             lower = tau_hat - 1.96 * sigma_hat)
# Create a data frame with predicted values, lower and upper bounds of the 95% confidence interval, and covariates
cf_plot$Incm_mdn <- covariates[, 1]
cf_plot$saving_dummy <- covariates[, 2]

# Plot the predicted values as a line
median_inc_plot <- ggplot(cf_plot, aes(x = factor(Incm_mdn), y = tau_hat)) +
  geom_point() +
  # Add shaded area for the 95% confidence interval
  geom_errorbar(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  # Add a horizontal line at y=0
  # Add axis labels
  scale_x_discrete(labels= c("Below Median", "Above Median")) +
  xlab("Income") +
  ylab("tau_hat")+
  theme_classic()
# Print summary of variables


  # Plot the predicted values as a line
  median_sav_plot <- ggplot(cf_plot, aes(x = saving_dummy, y = tau_hat)) +
    geom_point() +
    # Add shaded area for the 95% confidence interval
    geom_errorbar(aes(ymin = lower, ymax = upper), alpha = 0.2) +
    # Add a horizontal line at y=0
    # Add axis labels
    scale_x_discrete(labels= c("No Savings", "Savings")) + 
  xlab("Savings") +
    ylab("tau_hat")
  # Print summary of variables


# CATE on full sample
cf_cate <- average_treatment_effect(cf_1, target.sample = "all")

# CATT on full sample
cf_catt <- average_treatment_effect(cf_1, target.sample = "treated")

covariates_imp <- covariates %>% 
  as.data.frame() %>% 
  select(c("Incm_mdn", "saving_dummy")) %>% as.matrix()

cace_predict <- best_linear_projection(cf_1, covariates_imp, vcov.type = "HC3")
summary(cace_predict)

plot(X.test[, 1], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[, 1], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[, 1], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 1)

# Calculate Average Treatment Effect
  # Estimate Average Treatment Effects and Error
#average_treatment_effect(cf_multi, method = "AIPW",) # AIPW is for doubly robust errors

#tune_pa(covariates, outcome, treatment, cluster= cf_data$Iddir)

  # Summary 
#summary(cf_multi)


## 4. Model Evaluation ########################################################

# Multi_Arm Model


  #Test variable importance
kable(multi_cf_1_vi_t, "latex") %>%
  kable_styling(bootstrap_options = NULL)


  # variable importance
kable(multi_cf_1_vi, "latex") %>%
  kable_styling(bootstrap_options = NULL)

  # CATE Plots against Income
Income_plots 

ggsave(filename = "Income_plots", plot = Income_plots , device = "png", path = "2_Analysis//C_Output//")

  # CATE Plots against Savings
savings_plots 

ggsave(filename = "savings_plots", plot = savings_plots, device = "png", path = "2_Analysis//C_Output//")
  # ATE of each treatment on insurance uptake 
kable(multi_ate, "latex") %>%
  kable_styling(bootstrap_options = NULL)

  # Could not calculate heterogenous effects, coding limitations 
# Plot data by CATE instead. cannot doubly robust resulst to obtain HTE

#Simple causal model - on Iddir impact 
cf_import

  # ATE
cf_ate 

  # HTE
cace_predict 

  # Residual plot

  # Variable Importance Plot

  # Conditional Effect Plot

## 5. Plots and Graphs ########################################################

## Script End #################################################################


